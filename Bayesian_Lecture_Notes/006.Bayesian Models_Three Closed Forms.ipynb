{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b6761e-3763-45c8-874d-f16e2b8cb7d9",
   "metadata": {},
   "source": [
    "# <font color=\"darkblue\"> Bayesian Models_Closed Form\n",
    "\n",
    "## <font color=\"darkgreen\">Introduction\n",
    "\n",
    "Let us begin with a data model represented by a probability distribution $f(\\mathbf{X}|θ)$\n",
    "We prefer likelihood notion $\\mathscr{L}(\\theta|\\mathbf{X})=\\prod_{i=1}^n f(x_i|\\theta)$ is known but our interest is to know about the random unknown $\\theta$. The Main aspect of Bayesian procedure is considering $\\theta$ as a random variable in a model. The main objective of building a model is to know / infer about the unknown parameter\n",
    "\n",
    "Bayesian appraoch assumes distribution for parameters, it has to work with two distributions prior and posterior for θ; before and after having data. Bayes Theorem provides a simplistic way to move from prior to posterior; nevertheless, Bayesian methods and Bayes Theorem are not necessarily same. A nice treatment of this distinction can be found in many texts such as **Statistical Rethinking: A Bayesian Course with Examples in R and Stan Richard McElreath**\n",
    "\n",
    "This notes explains three basic Bayesian model that follow the strucutre of Bayes Theorem. It may be helpful if we recall few terms for better understanding\n",
    "\n",
    "### <font color=\"darkred\"> Keywords\n",
    "\n",
    "1. Abstraction of a situation - Random variable\n",
    "1. Distributions for random variables\n",
    "1. Parameters\n",
    "1. Summaries from a distribution\n",
    "1. Probability computations about a random variable\n",
    "1. Likelihood function\n",
    "\n",
    "### <font color=\"darkred\"> Conventional Symbols\n",
    "\n",
    "$\\theta:$ Parameter\n",
    "\n",
    "$f(\\mathbf{X}|\\theta):$  Probability density/mass function\n",
    "\n",
    "$\\mathscr{L}(\\theta|\\mathbf{X}):$  Likelihood function\n",
    "\n",
    "$l(\\theta|\\mathbf{X}) = ln[\\mathscr{L}(\\theta|\\mathbf{X})]:$ Log Likelihood function (useful in a technical context; for quick reading, please ignore)\n",
    "\n",
    "### <font color=\"darkred\"> Three Basic Models\n",
    "\n",
    "In most of the data modeling, three types of data are predominent; Qualitative, Quantitative and Count. First one is usually represented by categorical variable and the second one is by a metric/numeric variable. Here, we consider fundamental distributions for these three cases.\n",
    "\n",
    "**Binary categorical variable:** Parameter is proportion of success and data is the number of times success event happened out of some fixed number of trials\n",
    "\n",
    "**Metric (Numeric) variable:** A quantitative measure in nature which may be summarized by relevent quantities such as mean, percentiles etc. These summary measures may be considered as parameter of interest\n",
    "\n",
    "**Count variable:** A variable that invloves counting a quantity; that may be vary sparsely occuring or it may not be possible to predict the interval between to consective occurance of that event\n",
    "\n",
    "When there is a data set related to a study that has unknown parameters, the whole idea in building a model is to know about the parameter through its posterior distribution . In case we know the mathematical form of the posterior distribution, then summaries or probabilistic behaviour of the parameter is very direct. One advantage of knowing the mathematical form is to use the closed form formula to find mean or variance etc.\n",
    "\n",
    "### <font color=\"darkred\"> Binomial Distribution\n",
    "\n",
    "To begin with, we consider a one dimensional binary categorical data with two levels. here, the quantity of interest could be proportion of these two levels in the population. If we have a sample of a fixed size of data (size, $n$) then $\\theta$ could be proportion of success event (related to one level) and $1−\\theta$ is proportion of failure (another level of the binary variable).\n",
    "\n",
    "Such problems are modelled as Bernoulli trials; that is, if $\\mathbf{X}$  is the number of successes out of n Bernoulli trials, then we have \n",
    "\n",
    "**Data model:**\n",
    "\n",
    "$$\\mathbf{X} \\sim \\text{Binomial}(n,\\theta)$$ with the range of $\\mathbf{X}$ is $\\mathscr{A}_X=\\{0,1,2,\\cdots,n\\}$ and the parameter space is $0<\\theta<1$\n",
    "\n",
    "Since $\\theta$ is a continuous random variable ranges between $0$ and $1$, we choose Beta distribution as prior for $\\theta$.\n",
    "\n",
    "**Prior distribution:**\n",
    "\n",
    "$$\\theta\\,\\sim \\text{Beta}(a,b)$$ where $a,b>0$\n",
    "\n",
    "**Posterior distribution:**\n",
    "\n",
    "$$\\theta|\\mathbf{x} \\,\\sim \\text{Beta}(a+x,n−x+b)$$ a Beta distribution with parameters $a+x$\n",
    "and $n−x+b$\n",
    "\n",
    "Once the posterior is known in mathematically tractable form, then we can find its (selected) summaries using respective expressions.\n",
    "\n",
    "Mean of the posterior distribution is given by\n",
    "\n",
    "$$E[\\theta|\\mathbf{X}]=\\frac{a+x}{n+a+b}$$\n",
    "\n",
    "More about this mean\n",
    "\n",
    "$$E[\\theta|\\mathbf{X}]=\\frac{x+a}{n+a+b}$$\n",
    "\n",
    "$$=\\frac{x}{n+a+b}+\\frac{a}{n+a+b}$$\n",
    "\n",
    "$$=\\frac{n}{n+a+b}\\frac{x}{n}+\\frac{a+b}{n+a+b}\\frac{a}{a+b}$$\n",
    "\n",
    "$$=[\\frac{n}{n+a+b}]\\frac{x}{n}+[1-\\frac{n}{n+a+b}]\\frac{a}{a+b}$$\n",
    "\n",
    "$$=\\alpha \\frac{x}{n}+(1−\\alpha)\\frac{a}{a+b}$$  where $0 ≤ \\alpha ≤ 1$\n",
    "\n",
    "Hence, posterior mean is a **weighted Arithmetic Mean** of prior mean $\\frac{a}{a+b}$ and sample mean $\\frac{x}{n}$ with weights $\\alpha=\\frac{n}{n+a+b}$ and $1−\\alpha$. In the absence of no successes $x = 0$ then posterior mean equals prior mean\n",
    "\n",
    "**Other summaries:**\n",
    "\n",
    "Posterior Variance is \n",
    "\n",
    "$$V[\\theta|\\mathbf{X}]=\\frac{(x+a)(n−x+b)}{(n+a+b)^2(n+a+b+1)}$$\n",
    "\n",
    "Posterior Mode (MAP) \n",
    "\n",
    "$$\\frac{x+a-1}{n+a+b-2}$$\n",
    "\n",
    "we should ensure to get a positive value in the formula for mode ; this constraints the choice of the prior parameters a and b\n",
    "\n",
    "### <font color=\"darkred\">Normal Distribution\n",
    "\n",
    "Assume a measurable variable and the quantity of interest (parameter) is average of the variable. This may be modelled with a Gaussian (Normal) distribution\n",
    "\n",
    "**Data model:**\n",
    "\n",
    "$$\\mathbf{X} \\sim \\text{Normal}(\\mu,\\sigma^2)$$ with the range of $\\mathbf{X}$ is $\\mathscr{A}_X=(-\\infty,\\,\\infty)$ and the parameter space is $-\\infty<\\mu<\\infty$. \n",
    "\n",
    "Assumption of this model: $\\sigma^2$ is known \n",
    "\n",
    "The parameter $\\mu$ is continuous random variable, ranges between $-\\infty$ and $\\infty$. We choose Normal distribution as a prior for $\\mu$.\n",
    "\n",
    "**Prior distribution:**\n",
    "\n",
    "Normal distribution with parameters $\\delta,\\tau^2$\n",
    "\n",
    "$$\\mu\\sim \\,\\text{Normal}(\\eta,\\tau^2)$$ where $-\\infty <\\eta < \\infty$ and $\\tau^2>0$\n",
    "\n",
    "**Posterior distribution:**\n",
    "\n",
    "$$\\mu|\\mathbf{X} \\sim\\,\\text{Normal}(\\frac{B}{A},\\frac{1}{A})$$ where $A=\\frac{n}{\\sigma^2}+\\frac{1}{\\tau^2}; \\,B=(\\frac{n\\bar{x}}{\\sigma^2}+\\frac{\\eta}{\\tau^2})$\n",
    "\n",
    "Posterior Mean:\n",
    "\n",
    "$$E(\\mu|\\mathbf{X})=\\frac{B}{A}=\\frac{\\frac{1}{\\frac{\\sigma^2}{n}}\\bar{x}+\\frac{1}{\\tau^2}\\eta}{\\frac{1}{\\frac{\\sigma^2}{n}}+\\frac{1}{\\tau^2}}$$\n",
    "\n",
    "$$=\\frac{w_1}{w_1+w_2}\\bar{x}+\\frac{w_2}{w_1+w_2}\\eta$$\n",
    "\n",
    "That is posterior mean is a weighted mean of data precision $(w_1=\\frac{1}{\\frac{\\sigma^2}{n}})$  and prior precision $(w_2=\\frac{1}{\\tau^2})$\n",
    "\n",
    "Posterior Variance:\n",
    "\n",
    "$$V(\\mu|\\mathbf{X})=\\frac{1}{A}=\\frac{1}{\\frac{\\sigma^2}{n}+\\frac{1}{\\tau^2}}$$\n",
    "\n",
    "Equivalently, posterior precision is the sum of prior precision and data precision\n",
    "\n",
    "$\\frac{1}{\\frac{\\sigma^2}{n}}+\\frac{1}{\\tau^2}$\n",
    "\n",
    "### <font color=\"darkred\">Poisson Distribution\n",
    "\n",
    "Consider an iid sample  $x_1,x_2,\\cdots\\cdots\\cdots,x_n$ of a count variable $X$ that takes only non-negative integers $\\{0,1,2,3,\\cdots,\\cdots\\}$ Main interest is to infer about mean count; hence data is realized as count in an instant of the random variable X and quantity of interest (parameter) is to estimate average count $\\theta$. We shall model using Poisson distribution, one of the distributions for count data modeling.\n",
    "\n",
    "**Data Model:**\n",
    "\n",
    "$$\\mathbf{X} \\sim \\text{Poisson}(\\theta)$$ with the range of $\\mathbf{X}$ is $\\mathscr{A}_X=\\{0,1,2,\\cdots\\cdots\\cdots\\}$ and the parameter space is $\\theta>0$. \n",
    "\n",
    "The parameter $\\theta$ is continuous random variable, its range is $(0,\\infty)$. We choose Gamma distribution as a prior for $\\theta$.\n",
    "\n",
    "**Prior distribution:**\n",
    "\n",
    "$$\\theta\\sim\\text{Gamma}(\\alpha,\\beta)$$ where $\\alpha>0$ is the shape parameter, $\\beta>0$ is the scale parameter \n",
    "\n",
    "**Posterior distribution:**\n",
    "\n",
    "$$\\theta|\\mathbf{X}\\sim\\text{Gamma}(a, b)$$ where $a = \\sum_{i=1}^n x_i+\\alpha$ is the shape parameter and $b=\\frac{1}{n+\\frac{1}{\\beta}}$ is the scale parameter \n",
    "\n",
    "Posterior mean:\n",
    "\n",
    "$E[\\theta|\\mathbf{X}]= \\frac{\\sum_{i=1}^n x_i+\\alpha} {n+\\frac{1}{\\beta}}$\n",
    "\n",
    "$= \\frac{n}{n+\\frac{1}{\\beta}}\\bar{x}+\\frac{\\frac{1}{\\beta}}{n+\\frac{1}{\\beta}}\\alpha\\beta$\n",
    "\n",
    "\n",
    "Hence, posterior mean is weighted mean of sample mean (data) and prior mean with weights are sample size and prior rate.\n",
    "\n",
    "## <font color=\"darkgreen\"> Probability Computations\n",
    "\n",
    "If $X$ is a random variable, it is customary to calculate probabilities involving $X$ or functions of $X$, in addition to standard summaries such as the mean and variance. In the Bayesian context, since a parameter $\\theta$ is treated as a random variable, it is possible to compute probabilities about $\\theta$ using its prior or posterior distribution.\n",
    "\n",
    "Probabilities involving a discrete / continuous random variable (using the PDF / PMF) are computed in a different notion\n",
    "\n",
    "1. In the case of a DRV, PMF itself is the probability of the RV at a given value \n",
    "\n",
    "2. If the RV is a CRV, then probability at an exact point is zero. It is calculated as **Area Under the Probability curve between two values of the RV**\n",
    "\n",
    "- Assume X is a 1-D CRV in the range $(L, H)$ with pdf $f(x|\\theta)$; let $a$ and $b$ be two numbers in $(L, H)$ then we can compute \n",
    "\n",
    "  - $Pr[X>a]=\\int_a^H f(x|\\theta)dx$\n",
    "\n",
    "  - $Pr[X<b]=\\int_L^b f(x|\\theta)dx$\n",
    "\n",
    "  - $Pr[a<X<b]=\\int_a^b f(x|\\theta)dx$\n",
    "\n",
    "For example, one might be interested in the probability that $\\theta$, a proportion parameter in a Binomial model, exceeds some desired value, say $\\theta_0$\n",
    "\n",
    "\n",
    "$$Pr[(\\theta|X)>\\theta_0]=\\int_{\\theta_0}^1 \\pi(\\theta|X) d\\theta$$\n",
    "\n",
    "Since the posterior $\\pi(\\theta|X)$ of $\\theta$ is a beta distribution, \n",
    "\n",
    "$$= \\int_{\\theta_0}^1 \\frac{\\theta^{a+x-1}(1-\\theta)^{n-x+b-1}}{\\beta(a+x,n-x+b)} ~d\\theta$$ a,x,n,b are known\n",
    "\n",
    "This is **Incomplete Beta Function**  This can be referred at https://mathworld.wolfram.com/IncompleteBetaFunction.html, https://en.wikipedia.org/wiki/Beta_function etc\n",
    "\n",
    "Similarly, the goal may be to find the **Probability** between two values of \\(\\theta\\) using the posterior distribution\n",
    "\n",
    "$\\theta$ lies between chosen two values $\\theta_1~ \\&~ \\theta_2$. That is to find\n",
    "\n",
    "$$Pr[\\theta_1 < (\\theta|X) < \\theta_2]$$\n",
    "\n",
    "In Bayesian theory, the parameter is treated as a random variable, allowing for **Interval Estimation** to be defined directly in terms of the parameter. The preceding computation provides a method for finding **Credible Intervals**.\"\n",
    "\n",
    "\n",
    "That is for a pre-specified (desired) probability $\\alpha$ we need to find two values of $a~ \\& ~b$ such that $Pr[a < \\theta < b]=\\alpha$\n",
    "\n",
    "One way is to use Inverse-CDF to find the $a ~ \\& ~ b$; that is,\n",
    "\n",
    "1. Find $\\frac{\\alpha}{2}$\n",
    "\n",
    "1. $1-\\frac{\\alpha}{2}$\n",
    "\n",
    "1. Use Inverse-CDF with these two values to find $a ~ \\& ~ b$ using\n",
    "\n",
    "$$\\int_{-\\infty}^a\\pi(\\theta|\\mathbf{X})\\, d\\theta=\\frac{\\alpha}{2}$$\n",
    "\n",
    "$$\\int_b^{\\infty}\\pi(\\theta|\\mathbf{X})\\, d\\theta=1-(\\frac{\\alpha}{2})$$\n",
    "\n",
    "assuming the paramete $\\theta$ is a 1-D continuous random variable, limits can be adjusted according to the the parameter's space $\\mathscr{A}_{\\theta}$ \n",
    "\n",
    "### <font color=\"darkviolet\"> End Notes\n",
    "\n",
    "It can be observed that computations are necessary in all probability calculations involving the posterior (and, in some cases, prior) distribution of the parameter. These calculations may arise in summaries such as the median or mode, among others. Most of them do not have closed-form solutions. This highlights the need to study suitable numerical methods as part of Bayesian modeling.\n",
    "\n",
    "### <font color=\"darkblue\">Final Remarks\n",
    "\n",
    "An attempt is made to exemplify Bayesian modelling through three basic models that have closed form posterior distribution when conjugate prior is used. However, articulation of prior distribution is a different learning curve that requires fluent modeling practices. A lot of examples available in standard texts and in many research papers. Enthusiastic Bayesian practitioner could explore this in their own Bayesin modeling. Nevertheless, following keywords are primarily important in Bayesian modeling\n",
    "\n",
    "1. Prior and its variants\n",
    "1. Bayes formula\n",
    "1. Posterior\n",
    "1. Bayesian Estimator\n",
    "1. Bayesian Estimate\n",
    "\n",
    "Also to note that always it may not be possible to have a tractable posterior. Even one such exists, summaries may not be in closed form. Hence we need to seek suitable alternatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
