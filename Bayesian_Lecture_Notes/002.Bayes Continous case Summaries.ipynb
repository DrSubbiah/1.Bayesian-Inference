{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab68083-69f3-491b-8642-4ca244a6b939",
   "metadata": {},
   "source": [
    "# <font color=\"darkblue\"> Bayes Theorem\n",
    "\n",
    "## <font color=\"darkred\"> **Bayes Theorem for the Continuous Case (1D)**\n",
    "\n",
    "For the continuous case, where $ \\theta $ is a continuous random variable, Bayes' Theorem is analogous to discrete case, but with integrals replacing sums.\n",
    "\n",
    "**Bayes' Theorem for a continuous random variable $ \\theta $ is:**\n",
    "\n",
    "$$\n",
    "P(\\theta | \\text{X}) = \\frac{P(\\text{X} | \\theta) P(\\theta)}{P(\\text{X})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(\\theta | \\text{X}) $ is the **posterior distribution** of $ \\theta $ given the data.\n",
    "- $ P(\\text{X} | \\theta) $ is the **likelihood** function, representing the probability of the data given $ \\theta $.\n",
    "- $ P(\\theta) $ is the **prior distribution** of $ \\theta $.\n",
    "- $ P(\\text{X}) $ is the **marginal likelihood**, which in the continuous case is the integral over all possible values of $ \\theta $:\n",
    "\n",
    "$$\n",
    "P(\\text{X}) = \\int P(\\text{X} | \\theta) P(\\theta) d\\theta\n",
    "$$\n",
    "\n",
    "The denominator, $ P(\\text{X}) $, ensures that the posterior distribution integrates to 1, normalizing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c44c1-26a1-4e19-90ca-699b2cba9f4b",
   "metadata": {},
   "source": [
    "# <font color=\"darkblue\"> Methods for Estimating Parameters from a Bayesian Posterior Distribution\n",
    "\n",
    "In the Bayesian framework, after combining the likelihood and prior using Bayes' Theorem, the **posterior distribution** provides a complete description of our updated beliefs about the parameters of interest, given the observed data. Estimating parameters from the posterior distribution can be done using different methods, each with its own implications and interpretations. Below, we discuss some of the most commonly used methods for parameter estimation in the Bayesian framework.\n",
    "\n",
    "## 1. **Point Estimates from the Posterior Distribution**\n",
    "\n",
    "In Bayesian inference, we often seek a **point estimate** of the parameters. A point estimate summarizes the central tendency of the posterior distribution. Common point estimators include:\n",
    "\n",
    "### a. **Maximum A Posteriori (MAP) Estimation**\n",
    "\n",
    "- **Definition**: The MAP estimate is the value of the parameter that maximizes the posterior distribution.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\hat{\\theta}_{MAP} = \\arg \\max_{\\theta} P(\\theta | \\text{data}) = \\arg \\max_{\\theta} P(\\text{data} | \\theta) P(\\theta)\n",
    "  $$\n",
    "\n",
    "### Description of $ \\arg \\max_{\\theta} $\n",
    "\n",
    "The notation $ \\arg \\max_{\\theta} $ refers to the value of $ \\theta $ that maximizes a function. \n",
    "\n",
    "- $ \\arg $ stands for \"argument,\" which represents the input or parameter of a function.\n",
    "- $ \\max $ refers to \"maximum,\" indicating the largest value the function can achieve.\n",
    "\n",
    "Thus, $ \\arg \\max_{\\theta} f(\\theta) $ denotes the value of $ \\theta $ that results in the maximum value of the function $ f(\\theta) $. \n",
    "\n",
    "In simpler terms, it is the input $ \\theta $ that maximizes the function $ f(\\theta) $.\n",
    "\n",
    "  - This approach is similar to **Maximum Likelihood Estimation (MLE)** but incorporates prior knowledge (through the prior distribution).\n",
    "- **Implications**: \n",
    "  - MAP estimation provides a **point estimate** that balances the likelihood of the data and the prior information.\n",
    "  - The choice of prior heavily influences the estimate, especially in cases with limited data.\n",
    "  - MAP estimation is widely used when the posterior distribution is not easy to sample from or when a single estimate is required.\n",
    "\n",
    "#### b. **Posterior Mean (Bayesian Average)**\n",
    "\n",
    "- **Definition**: The posterior mean is the expected value of the parameter with respect to the posterior distribution.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  \\hat{\\theta}_{mean} = \\mathbb{E}[\\theta | \\text{data}] = \\int \\theta P(\\theta | \\text{data}) d\\theta\n",
    "  $$\n",
    "- **Implications**:\n",
    "  - The posterior mean is a **central tendency** estimate that takes into account the entire posterior distribution.\n",
    "  - It is generally more sensitive to the data than the prior, especially when the sample size is large.\n",
    "  - The posterior mean is **ideal** when the posterior distribution is symmetric and unimodal, as it balances the influence of both the data and the prior.\n",
    "\n",
    "#### c. **Posterior Median**\n",
    "\n",
    "- **Definition**: The posterior median is the value that divides the posterior distribution into two equal parts.\n",
    "- **Implications**:\n",
    "  - The posterior median is less sensitive to extreme values in the distribution compared to the posterior mean, making it a more **robust** estimator in cases where the posterior distribution is skewed or contains outliers.\n",
    "\n",
    "### 2. **Interval Estimates**\n",
    "\n",
    "In addition to point estimates, Bayesian inference provides a natural way to quantify **uncertainty** through interval estimates, reflecting the spread or confidence in the parameter estimates.\n",
    "\n",
    "#### a. **Credible Interval**\n",
    "\n",
    "- **Definition**: A **credible interval** is the Bayesian counterpart of a confidence interval. It is the interval within which the parameter lies with a certain probability, given the data and prior.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  P(\\theta \\in [a, b] | \\text{data}) = \\int_a^b P(\\theta | \\text{data}) d\\theta\n",
    "  $$\n",
    "- **Implications**:\n",
    "  - A 95% credible interval represents the range within which the true parameter lies with 95% probability, given the data and the prior.\n",
    "  - Credible intervals have a **direct probabilistic interpretation**: there is a 95% probability that the true parameter lies within this range.\n",
    "  - Credible intervals can be used to represent **uncertainty** in parameter estimation, unlike frequentist confidence intervals.\n",
    "\n",
    "### 3. **Sampling Methods**\n",
    "\n",
    "For complex models or when it is difficult to derive an analytical solution, we often resort to **sampling methods** to estimate parameters from the posterior distribution. These methods are useful when the posterior distribution is not easy to express in closed form.\n",
    "\n",
    "#### a. **Markov Chain Monte Carlo (MCMC)**\n",
    "\n",
    "- **Definition**: MCMC is a family of algorithms used to generate samples from the posterior distribution. The most widely used MCMC method is the **Metropolis-Hastings** algorithm, but **Gibbs sampling** is also commonly used in hierarchical models.\n",
    "- **Implications**:\n",
    "  - MCMC provides a way to **sample** from the posterior distribution and make inferences about parameters by analyzing these samples.\n",
    "  - After running the MCMC algorithm, we can estimate the posterior mean, median, credible intervals, or other statistics from the generated samples.\n",
    "  - MCMC is computationally intensive, especially for complex models, but it allows us to explore complicated, high-dimensional posterior distributions.\n",
    "\n",
    "#### b. **Importance Sampling**\n",
    "\n",
    "- **Definition**: In importance sampling, we draw samples from a **proposal distribution** and use them to approximate expectations under the true posterior distribution.\n",
    "- **Implications**:\n",
    "  - Importance sampling is useful when direct sampling from the posterior is difficult.\n",
    "  - The efficiency of importance sampling depends on how well the proposal distribution approximates the posterior.\n",
    "\n",
    "### 4. **Implications of the Estimation Methods**\n",
    "\n",
    "The choice of method for parameter estimation depends on various factors:\n",
    "- **Data Size**: For large datasets, the posterior mean tends to be the most accurate, as the influence of the prior diminishes.\n",
    "- **Model Complexity**: Complex models may require MCMC or other sampling methods to estimate the posterior.\n",
    "- **Prior Information**: The strength and form of the prior will influence the estimation, especially in cases with small data.\n",
    "- **Computational Resources**: Sampling methods like MCMC can be computationally expensive, so simpler methods like MAP or posterior mean may be preferred in resource-constrained environments.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Estimating parameters in the Bayesian framework allows us to incorporate prior knowledge and quantify uncertainty. The choice of estimation method—whether point estimates like MAP and posterior mean, interval estimates like credible intervals, or sampling methods like MCMC—depends on the complexity of the problem, the available data, and the computational resources. Each method provides a different perspective on the posterior distribution, and the right approach depends on the context and goals of the analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
